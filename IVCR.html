<!DOCTYPE html>
<html lang="en">
<head>
    <meta charset="UTF-8">
    <title>IVCR-200K: A Large-Scale Multi-turn Dialogue Benchmark for Interactive Video Corpus Retrieval</title>
    <link href="https://fonts.googleapis.com/css2?family=Open+Sans&display=swap" rel="stylesheet">
    <link rel="stylesheet" href="css/style.css" media="screen"/>
</head>
<body>
    <div class="container">
        <h1 class="project-title">IVCR-200K: A Large-Scale Multi-turn Dialogue Benchmark for Interactive Video Corpus Retrieval</h1>

        <div class="authors">
            <p>Ning Han<sup>1*</sup>,Yawen Zeng<sup>2*</sup>, Shaohua Long<sup>1</sup>, Chengqing Li<sup>1</sup>, Sijie Yang<sup>1</sup>, Dun Tan<sup>1</sup>,<p>Zemin Liu<sup>3</sup>,<p>Jingjing Chen<sup>4</sup>,</p>
			<p><sup>1</sup> Xiangtan University</p>
			<p><sup>2</sup> ByteDance</p>
			<p><sup>3</sup> Zhejiang University</p>
			<p><sup>4</sup> Fudan University</p>
        </div>

        <div class="icon-container">
            <ul class="icon-list">
                <li class="icon-item">
                    <a href="https://arxiv.org/abs/2309.13925">
                        <img src="img/file-pdf.png" alt="Paper">
                        <h4><strong>Paper</strong></h4>
                    </a>
                </li>
                <li class="icon-item">
                    <a href="https://github.com/Xuange923/Surveillance-Video-Understanding">
                        <img src="img/github.png" alt="Code">
                        <h4><strong>Code and Dataset</strong></h4>
                    </a>
                </li>
            </ul>
        </div>

        <h2>Abstract</h2>
        <p>Surveillance videos are an essential component of daily life with various critical applications, particularly in public security. However, current surveillance video tasks mainly focus on classifying and localizing anomalous events. Existing methods are limited to detecting and classifying the predefined events with unsatisfactory semantic understanding, although they have obtained considerable performance.  
To address this issue, we propose a new research direction of surveillance video-and-language understanding, and construct the first multimodal surveillance video dataset. We manually annotate the real-world surveillance dataset UCF-Crime with fine-grained event content and timing. Our newly annotated dataset, UCA UCF-Crime Annotation), contains 23,542 sentences, with an average length of 20 words, and its annotated videos are as long as 110.7 hours.
Furthermore, we benchmark SOTA models for four multimodal tasks on this newly created dataset, which serve as new baselines for surveillance video-and-language understanding. Through our experiments, we find that mainstream models used in previously publicly available datasets perform poorly on surveillance video, which demonstrates the new challenges in surveillance video-and-language understanding. To validate the effectiveness of our UCA, we conducted experiments on multimodal anomaly detection. The results demonstrate that our multimodal surveillance learning can improve the performance of conventional anomaly detection tasks. All the experiments highlight the necessity of constructing this dataset to advance surveillance AI. </p>
       
	<hr>	
		<h2>Interactive Video Corpus Retrieval Dataset</h2>		
		<h3>Dataset Collection and Annotation</h3>
		<h3>Dataset Analysis.</h3>
		<h3>Parts of Speech Distribution</h3>
		<hr>

        <h2>Experimental Tasks</h2>
		<p>We conducted four types of experiments on our dataset:</p>
        <ul style="font-size: 18px; line-height: 1.6; ">
            <li>Temporal Sentence Grounding in Videos (TSGV): This task focuses on temporal activity localization in a video based on a language query.</li>
            <li>Video Captioning (VC): Understanding a video clip and describing it with language.</li>
            <li>Dense Video Captioning (DVC): Involves generating the temporal localization and captioning of dense events in an untrimmed video.</li>
            <li>Multimodal Anomaly Detection (MAD): Utilizes captions as a text feature source to enhance traditional anomaly detection in complex surveillance videos.</li>
        </ul>
		<hr>

		<h2>Visualizations</h2>
		<p>To better understand the dataset and the experimental outcomes, the following visualizations are included:</p>
		
		<div class="visualization">
		    <h3>TSGV Visualization</h3>
		    <p>Example by MMN.</p>
		    <img src="https://i.postimg.cc/mhfbhhc7/fig-tsgv-2.jpg" alt="TSGV Visualization">
		</div>
		
		<div class="visualization">
		    <h3>VC Visualization</h3>
		    <p>Example by SwinBert.</p>
		    <img src="https://i.postimg.cc/pVqj6Hwk/fig-vc.jpg" alt="VC Visualization">
		</div>
		
		<div class="visualization">
		    <h3>DVC Visualization</h3>
		    <p>Example by PDVC.</p>
		    <img src="https://i.postimg.cc/wjd95twr/fig-dvc.jpg" alt="DVC Visualization">
		</div>
		
		<div class="visualization">
		    <h3>MAD Captioning Results</h3>
		    <p>Examples of different video captioning results.</p>
		    <img src="https://i.postimg.cc/V5PPFTWt/fig-mad.jpg" alt="MAD Captioning Results">
		</div>
<hr>
			
		<div class="ucf-crime-dataset">
			<h2>Original UCF-Crime Dataset Reference</h2>
			<p>Our UCA dataset is built upon the foundational UCF-Crime dataset. For those interested in exploring the original data, the UCF-Crime dataset can be downloaded directly from this link: <a href="http://www.crcv.ucf.edu/data1/chenchen/UCF_Crimes.zip" download="UCF_Crimes.zip">Download zip</a>&ensp; URL: <a href="http://www.crcv.ucf.edu/data1/chenchen/UCF_Crimes.zip">www.crcv.ucf.edu/data1/chenchen/UCF_Crimes.zip</a> </p>
			<p>Additionally, further details about the UCF-Crime project are available on their official website: <a href="https://www.crcv.ucf.edu/research/real-world-anomaly-detection-in-surveillance-videos/">Visit here</a></p>
			<p>If you wish to reference the UCF-Crime dataset in your work, please cite the following paper:</p>
			<pre>
				@inproceedings{sultani2018real,
				title={Real-world anomaly detection in surveillance videos},
				author={Sultani, Waqas and Chen, Chen and Shah, Mubarak},
				booktitle={Proceedings of the IEEE conference on computer vision and pattern recognition},
				pages={6479--6488},
				year={2018}
				}
			</pre>
			<p>Each annotation in our UCA dataset is associated with a corresponding video in the original UCF-Crime dataset. Users interested in this dataset can easily match the videos to the annotation information after downloading.</p>
		</div>
<hr>
        <h2>Usage and Contact</h2>
        <p>Our dataset is exclusively available for academic and research purposes. Please feel free to contact the original authors for inquiries, suggestions, or collaboration proposals.</p>
        <h3>Citation</h3>
		<pre>
			@misc{yuan2023surveillance,
			      title={Towards Surveillance Video-and-Language Understanding: New Dataset, Baselines, and Challenges}, 
			      author={Tongtong Yuan and Xuange Zhang and Kun Liu and Bo Liu and Chen Chen and Jian Jin and Zhenzhen Jiao},
			      year={2023},
			      eprint={2309.13925},
			      archivePrefix={arXiv},
			      primaryClass={cs.CV}
			}
		</pre>
    </div>
</body>
</html>
